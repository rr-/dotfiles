#!/usr/bin/env python
# requires: openai tiktoken pyyaml prompt-toolkit xdg-base-dirs

import argparse
import atexit
import dataclasses as dc
import datetime
import json
import select
import sys
from decimal import Decimal
from pathlib import Path

import tiktoken
import yaml
from openai import OpenAI
from prompt_toolkit import HTML, PromptSession
from prompt_toolkit.enums import DEFAULT_BUFFER
from prompt_toolkit.filters import Condition, HasFocus
from prompt_toolkit.history import FileHistory
from prompt_toolkit.key_binding import KeyBindings, KeyPressEvent
from xdg_base_dirs import xdg_config_home

BASE = Path(xdg_config_home(), "ai")
CONFIG_FILE = BASE / "config.yaml"
HISTORY_FILE = BASE / "history"
SAVE_FILE = (
    BASE
    / "session-history"
    / (
        "chatgpt-session-"
        + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        + ".json"
    )
)
BASE_ENDPOINT = "https://api.openai.com/v1"


@dc.dataclass
class ModelInfo:
    name: str
    prompt: Decimal
    completion: Decimal
    tokens: int


MODEL_INFO = [
    ModelInfo(
        name="gpt-3.5-turbo",
        prompt=Decimal("0.001"),
        completion=Decimal("0.002"),
        tokens=4096,
    ),
    ModelInfo(
        name="gpt-3.5-turbo-1106",
        prompt=Decimal("0.001"),
        completion=Decimal("0.002"),
        tokens=16385,
    ),
    ModelInfo(
        name="gpt-3.5-turbo-0613",
        prompt=Decimal("0.001"),
        completion=Decimal("0.002"),
        tokens=16385,
    ),
    ModelInfo(
        name="gpt-3.5-turbo-16k",
        prompt=Decimal("0.001"),
        completion=Decimal("0.002"),
        tokens=16385,
    ),
    ModelInfo(
        name="gpt-4",
        prompt=Decimal("0.03"),
        completion=Decimal("0.06"),
        tokens=8192,
    ),
    ModelInfo(
        name="gpt-4-0613",
        prompt=Decimal("0.03"),
        completion=Decimal("0.06"),
        tokens=8192,
    ),
    ModelInfo(
        name="gpt-4-32k",
        prompt=Decimal("0.06"),
        completion=Decimal("0.12"),
        tokens=32768,
    ),
    ModelInfo(
        name="gpt-4-32k-0613",
        prompt=Decimal("0.06"),
        completion=Decimal("0.12"),
        tokens=32768,
    ),
    ModelInfo(
        name="gpt-4-1106-preview",
        prompt=Decimal("0.01"),
        completion=Decimal("0.03"),
        tokens=128000,
    ),
]


Message = dict[str, str]


def get_model_info(model_name: str) -> ModelInfo | None:
    for model_info in MODEL_INFO:
        if model_name == model_info.name:
            return model_info
    return None


@dc.dataclass
class ChatContext:
    client: OpenAI
    model: str
    temperature: float
    messages: list[Message] = dc.field(default_factory=list)

    @property
    def user_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] == "user"
        ]

    @property
    def completion_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] != "user"
        ]

    @property
    def prompt_tokens(self) -> int:
        return num_tokens_from_messages(self.user_messages, self.model)

    @property
    def completion_tokens(self) -> int:
        return num_tokens_from_messages(self.completion_messages, self.model)


@dc.dataclass
class Config:
    api_key: str = "INSERT API KEY HERE"
    model: str = "gpt-4o"
    temperature: float = 1.0


def load_config(config_file: Path) -> Config:
    """
    Read a YAML config file and returns its content as a dictionary.
    If the config file is missing, create one with default values.
    If the config file is present but missing keys, populate them with defaults.
    """
    # If the config file does not exist, create one with default configurations
    if not config_file.exists():
        config_file.parent.mkdir(exist_ok=True, parents=True)
        config_file.write_text(yaml.dump(dc.asdict(Config())))
        print(f"New config file initialized: {config_file}", file=sys.stderr)

    # Load existing config
    user_config = yaml.load(config_file.read_text(), Loader=yaml.FullLoader)

    # Fill missing values with defaults
    result = Config()
    for field in dc.fields(result):
        key = field.name
        keys_to_try = {key, key.replace("_", "-")}
        for user_key in keys_to_try:
            if user_key in user_config:
                setattr(result, key, user_config[user_key])
    return result


def save_history(context: ChatContext) -> None:
    """Save the conversation history in JSON format."""
    SAVE_FILE.parent.mkdir(exist_ok=True, parents=True)
    SAVE_FILE.write_text(
        json.dumps(
            {
                "model": context.model,
                "messages": context.messages,
                "prompt_tokens": context.prompt_tokens,
                "completion_tokens": context.completion_tokens,
            },
            indent=4,
            ensure_ascii=False,
        )
    )


def num_tokens_from_messages(messages: list[Message], model: str) -> int:
    """Returns the number of tokens used by a list of messages."""
    encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = 0
    for message in messages:
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
    return num_tokens


def calculate_expense(
    prompt_tokens: int,
    completion_tokens: int,
    prompt_pricing: Decimal,
    completion_pricing: Decimal,
) -> Decimal:
    """
    Calculate the expense, given the number of tokens and the pricing rates
    """
    return ((Decimal(prompt_tokens) / Decimal(1000)) * prompt_pricing) + (
        (Decimal(completion_tokens) / Decimal(1000)) * completion_pricing
    )


def display_expense(context: ChatContext) -> None:
    """
    Given the model used, display total tokens used and estimated expense
    """
    print(
        f"\nTotal tokens used: {context.prompt_tokens + context.completion_tokens}",
        file=sys.stderr,
    )

    if model_info := get_model_info(context.model):
        total_expense = calculate_expense(
            context.prompt_tokens,
            context.completion_tokens,
            model_info.prompt,
            model_info.completion,
        )
        print(f"Estimated expense: ${total_expense:.6f}", file=sys.stderr)
    else:
        print(
            f"No expense estimate available for model {context.model}",
            file=sys.stderr,
        )


def run_prompt(context: ChatContext, message: str) -> None:
    """Ask the user for input, build the request and perform it."""
    context.messages.append({"role": "user", "content": message})

    stream = context.client.chat.completions.create(
        model=context.model,
        messages=context.messages,
        temperature=context.temperature,
        stream=True,
    )

    try:
        output = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                print(chunk.choices[0].delta.content, end="", flush=True)
                output += chunk.choices[0].delta.content
        context.messages.append({"role": "assistant", "content": output})
    except KeyboardInterrupt:
        pass
    finally:
        stream.response.close()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c", "--context", nargs="+", help="Additional context"
    )
    parser.add_argument(
        "-C",
        "--context-path",
        type=Path,
        nargs="+",
        help="Path to a context file",
    )
    parser.add_argument("-m", "--model", help="Set the model")
    parser.add_argument(
        "-t",
        "--temperature",
        type=float,
        help="Set the temperature (default: 1)",
        default=1.0,
    )
    parser.add_argument(
        "-ml",
        "--multiline",
        action="store_true",
        help="Use the multiline input mode",
    )
    return parser.parse_args()


def stdin_has_data() -> bool:
    return select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], [])


def run_interactive(context: ChatContext, args: argparse.Namespace) -> None:
    is_multiline = [args.multiline]
    multiline = Condition(lambda: is_multiline[0])
    kb = KeyBindings()

    @kb.add("escape", "m")  # type: ignore
    def handle_alt_m(event: KeyPressEvent) -> None:
        is_multiline[0] = not is_multiline[0]

    @kb.add("c-m", filter=HasFocus(DEFAULT_BUFFER) & multiline)  # type: ignore
    def handle_newline_multiline(event: KeyPressEvent) -> None:
        event.app.current_buffer.newline()

    @kb.add("c-m", filter=HasFocus(DEFAULT_BUFFER) & ~multiline)  # type: ignore
    def handle_newline_single_line(event: KeyPressEvent) -> None:
        event.app.current_buffer.validate_and_handle()

    history = FileHistory(HISTORY_FILE)
    session = PromptSession(history=history, multiline=True, key_bindings=kb)

    def get_prompt() -> HTML:
        prompt = ">>>" if is_multiline[0] else ">"
        return HTML(
            f"<b>[{context.prompt_tokens + context.completion_tokens}] {prompt}</b> "
        )

    print("ChatGPT CLI", file=sys.stderr)
    print(f"Model in use: {context.model}", file=sys.stderr)

    atexit.register(display_expense, context=context)

    while True:
        try:
            print(file=sys.stderr)
            message = session.prompt(get_prompt)

            if message.lower() == ".exit":
                raise EOFError
            if message.lower() == "":
                raise KeyboardInterrupt

            run_prompt(context, message)
            print(file=sys.stderr)
            print(file=sys.stderr)
        except KeyboardInterrupt:
            continue
        except EOFError:
            break
    save_history(context)


def run_non_interactive(
    context: ChatContext, args: argparse.Namespace
) -> None:
    if args.multiline:
        run_prompt(context, sys.stdin.read())
    else:
        for line in sys.stdin.readlines():
            run_prompt(context, line)

    save_history(context)


def main() -> None:
    args = parse_args()

    try:
        config = load_config(CONFIG_FILE)
    except FileNotFoundError:
        print("Configuration file not found", file=sys.stderr)
        sys.exit(1)

    if args.model:
        config.model = args.model
    if args.temperature is not None:
        config.temperature = args.temperature

    messages: list[Message] = []
    if args.context_path:
        for path in args.context_path:
            print(f"Context file: {path.name}", file=sys.stderr)
            messages.append(
                {"role": "system", "content": path.read_text().strip()}
            )
    elif args.context:
        for text in args.context:
            print(f"Context: {text}", file=sys.stderr)
            messages.append({"role": "system", "content": text.strip()})

    context = ChatContext(
        client=OpenAI(api_key=config.api_key),
        model=config.model,
        temperature=config.temperature,
        messages=messages,
    )

    if stdin_has_data():
        run_non_interactive(context, args)
    else:
        run_interactive(context, args)


if __name__ == "__main__":
    main()
