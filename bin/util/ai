#!/bin/env python
# requires openai tiktoken pyyaml prompt-toolkit xdg-base-dirs

import argparse
import atexit
import datetime
import json
import logging
import os
import re
import sys
from dataclasses import dataclass, field
from decimal import Decimal
from pathlib import Path
from typing import Optional

import tiktoken
import yaml
from openai import OpenAI
from prompt_toolkit import HTML, PromptSession
from prompt_toolkit.history import FileHistory
from xdg_base_dirs import xdg_config_home

BASE = Path(xdg_config_home(), "ai")
CONFIG_FILE = BASE / "config.yaml"
HISTORY_FILE = BASE / "history"
SAVE_FILE = (
    BASE
    / "session-history"
    / (
        "chatgpt-session-"
        + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        + ".json"
    )
)
BASE_ENDPOINT = "https://api.openai.com/v1"

MODEL_INFO = {
    "gpt-3.5-turbo": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 4096,
    },
    "gpt-3.5-turbo-1106": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-3.5-turbo-0613": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-3.5-turbo-16k": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-4": {
        "prompt": Decimal("0.03"),
        "completion": Decimal("0.06"),
        "tokens": 8192,
    },
    "gpt-4-0613": {
        "prompt": Decimal("0.03"),
        "completion": Decimal("0.06"),
        "tokens": 8192,
    },
    "gpt-4-32k": {
        "prompt": Decimal("0.06"),
        "completion": Decimal("0.12"),
        "tokens": 32768,
    },
    "gpt-4-32k-0613": {
        "prompt": Decimal("0.06"),
        "completion": Decimal("0.12"),
        "tokens": 32768,
    },
    "gpt-4-1106-preview": {
        "prompt": Decimal("0.01"),
        "completion": Decimal("0.03"),
        "tokens": 128000,
    },
}


Message = dict


@dataclass
class ChatContext:
    model: str
    messages: list[Message] = field(default_factory=list)

    @property
    def user_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] == "user"
        ]

    @property
    def completion_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] != "user"
        ]

    @property
    def prompt_tokens(self) -> int:
        return num_tokens_from_messages(self.user_messages, self.model)

    @property
    def completion_tokens(self) -> int:
        return num_tokens_from_messages(self.completion_messages, self.model)


DEFAULT_CONFIG = {
    "api-key": "INSERT API KEY HERE",
    "model": "gpt-3.5-turbo",
    "temperature": 1,
    # 'max_tokens': 500,
}


def load_config(config_file: Path) -> dict:
    """
    Read a YAML config file and returns its content as a dictionary.
    If the config file is missing, create one with default values.
    If the config file is present but missing keys, populate them with defaults.
    """
    # If the config file does not exist, create one with default configurations
    if not config_file.exists():
        config_file.parent.mkdir(exist_ok=True, parents=True)
        config_file.write_text(
            yaml.dumps(DEFAULT_CONFIG, file, default_flow_style=False)
        )
        print(f"New config file initialized: {config_file}")

    # Load existing config
    user_config = yaml.load(config_file.read_text(), Loader=yaml.FullLoader)

    # Fill missing values with defaults
    return {**DEFAULT_CONFIG, **user_config}


def save_history(chat_context: ChatContext) -> None:
    """Save the conversation history in JSON format."""
    SAVE_FILE.parent.mkdir(exist_ok=True, parents=True)
    SAVE_FILE.write_text(
        json.dumps(
            {
                "model": chat_context.model,
                "messages": chat_context.messages,
                "prompt_tokens": chat_context.prompt_tokens,
                "completion_tokens": chat_context.completion_tokens,
            },
            indent=4,
            ensure_ascii=False,
        )
    )


def num_tokens_from_messages(messages: list[Message], model: str) -> int:
    """Returns the number of tokens used by a list of messages."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = 0
    for message in messages:
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
    return num_tokens


def calculate_expense(
    prompt_tokens: int,
    completion_tokens: int,
    prompt_pricing: float,
    completion_pricing: float,
) -> Decimal:
    """
    Calculate the expense, given the number of tokens and the pricing rates
    """
    expense = ((Decimal(prompt_tokens) / Decimal(1000)) * prompt_pricing) + (
        (Decimal(completion_tokens) / Decimal(1000)) * completion_pricing
    )

    expense = "{:.6f}".format(round(expense, 6))

    return expense


def display_expense(chat_context: ChatContext) -> None:
    """
    Given the model used, display total tokens used and estimated expense
    """
    print(
        f"\nTotal tokens used: {chat_context.prompt_tokens + chat_context.completion_tokens}"
    )

    if chat_context.model in MODEL_INFO:
        total_expense = calculate_expense(
            chat_context.prompt_tokens,
            chat_context.completion_tokens,
            MODEL_INFO[chat_context.model]["prompt"],
            MODEL_INFO[chat_context.model]["completion"],
        )
        print(f"Estimated expense: ${total_expense}")
    else:
        print(f"No expense estimate available for model {chat_context.model}")


def run_prompt(
    client: OpenAI, session: PromptSession, chat_context: ChatContext
) -> None:
    """Ask the user for input, build the request and perform it."""
    message = session.prompt(
        HTML(
            f"<b>[{chat_context.prompt_tokens + chat_context.completion_tokens}] >>> </b>"
        )
    )

    if message.lower() == ".exit":
        raise EOFError
    if message.lower() == "":
        raise KeyboardInterrupt

    chat_context.messages.append({"role": "user", "content": message})

    print()

    stream = client.chat.completions.create(
        model=chat_context.model,
        messages=chat_context.messages,
        stream=True,
    )

    try:
        output = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                print(chunk.choices[0].delta.content, end="", flush=True)
                output += chunk.choices[0].delta.content
        chat_context.messages.append({"role": "assistant", "content": output})
    except KeyboardInterrupt:
        pass
    finally:
        stream.response.close()

    print()
    print()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c", "--context", type=Path, nargs="+", help="Path to a context file"
    )
    parser.add_argument("-m", "--model", help="Set the model")
    parser.add_argument(
        "-ml",
        "--multiline",
        action="store_true",
        help="Use the multiline input mode",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    history = FileHistory(HISTORY_FILE)
    session = PromptSession(history=history, multiline=args.multiline)

    try:
        config = load_config(CONFIG_FILE)
    except FileNotFoundError:
        print("Configuration file not found")
        sys.exit(1)

    if args.model:
        config["model"] = args.model

    print("ChatGPT CLI")
    print(f"Model in use: {config['model']}")
    if args.context:
        for path in args.context:
            print(f"Context file: {path.name}")
            messages.append(
                {"role": "system", "content": path.read_text().strip()}
            )

    chat_context = ChatContext(model=config["model"])
    atexit.register(display_expense, chat_context=chat_context)

    client = OpenAI(api_key=config["api-key"])
    while True:
        try:
            run_prompt(client, session, chat_context)
        except KeyboardInterrupt:
            continue
        except EOFError:
            break
    save_history(chat_context)


if __name__ == "__main__":
    main()
