#!/bin/env python
# requires openai tiktoken pyyaml prompt-toolkit xdg-base-dirs

import argparse
import atexit
import datetime
import json
import sys
from dataclasses import dataclass, field
from decimal import Decimal
from pathlib import Path

import tiktoken
import yaml
from openai import OpenAI
from prompt_toolkit import HTML, PromptSession
from prompt_toolkit.history import FileHistory
from xdg_base_dirs import xdg_config_home

BASE = Path(xdg_config_home(), "ai")
CONFIG_FILE = BASE / "config.yaml"
HISTORY_FILE = BASE / "history"
SAVE_FILE = (
    BASE
    / "session-history"
    / (
        "chatgpt-session-"
        + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        + ".json"
    )
)
BASE_ENDPOINT = "https://api.openai.com/v1"

MODEL_INFO = {
    "gpt-3.5-turbo": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 4096,
    },
    "gpt-3.5-turbo-1106": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-3.5-turbo-0613": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-3.5-turbo-16k": {
        "prompt": Decimal("0.001"),
        "completion": Decimal("0.002"),
        "tokens": 16385,
    },
    "gpt-4": {
        "prompt": Decimal("0.03"),
        "completion": Decimal("0.06"),
        "tokens": 8192,
    },
    "gpt-4-0613": {
        "prompt": Decimal("0.03"),
        "completion": Decimal("0.06"),
        "tokens": 8192,
    },
    "gpt-4-32k": {
        "prompt": Decimal("0.06"),
        "completion": Decimal("0.12"),
        "tokens": 32768,
    },
    "gpt-4-32k-0613": {
        "prompt": Decimal("0.06"),
        "completion": Decimal("0.12"),
        "tokens": 32768,
    },
    "gpt-4-1106-preview": {
        "prompt": Decimal("0.01"),
        "completion": Decimal("0.03"),
        "tokens": 128000,
    },
}


Message = dict


@dataclass
class ChatContext:
    client: OpenAI
    model: str
    temperature: float
    messages: list[Message] = field(default_factory=list)

    @property
    def user_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] == "user"
        ]

    @property
    def completion_messages(self) -> list[Message]:
        return [
            message for message in self.messages if message["role"] != "user"
        ]

    @property
    def prompt_tokens(self) -> int:
        return num_tokens_from_messages(self.user_messages, self.model)

    @property
    def completion_tokens(self) -> int:
        return num_tokens_from_messages(self.completion_messages, self.model)


DEFAULT_CONFIG = {
    "api-key": "INSERT API KEY HERE",
    "model": "gpt-4o",
    "temperature": 1.0,
    # 'max_tokens': 500,
}


def load_config(config_file: Path) -> dict:
    """
    Read a YAML config file and returns its content as a dictionary.
    If the config file is missing, create one with default values.
    If the config file is present but missing keys, populate them with defaults.
    """
    # If the config file does not exist, create one with default configurations
    if not config_file.exists():
        config_file.parent.mkdir(exist_ok=True, parents=True)
        config_file.write_text(yaml.dump(DEFAULT_CONFIG))
        print(f"New config file initialized: {config_file}", file=sys.stderr)

    # Load existing config
    user_config = yaml.load(config_file.read_text(), Loader=yaml.FullLoader)

    # Fill missing values with defaults
    return {**DEFAULT_CONFIG, **user_config}


def save_history(context: ChatContext) -> None:
    """Save the conversation history in JSON format."""
    SAVE_FILE.parent.mkdir(exist_ok=True, parents=True)
    SAVE_FILE.write_text(
        json.dumps(
            {
                "model": context.model,
                "messages": context.messages,
                "prompt_tokens": context.prompt_tokens,
                "completion_tokens": context.completion_tokens,
            },
            indent=4,
            ensure_ascii=False,
        )
    )


def num_tokens_from_messages(messages: list[Message], model: str) -> int:
    """Returns the number of tokens used by a list of messages."""
    encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = 0
    for message in messages:
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
    return num_tokens


def calculate_expense(
    prompt_tokens: int,
    completion_tokens: int,
    prompt_pricing: float,
    completion_pricing: float,
) -> Decimal:
    """
    Calculate the expense, given the number of tokens and the pricing rates
    """
    expense = ((Decimal(prompt_tokens) / Decimal(1000)) * prompt_pricing) + (
        (Decimal(completion_tokens) / Decimal(1000)) * completion_pricing
    )

    expense = "{:.6f}".format(round(expense, 6))

    return expense


def display_expense(context: ChatContext) -> None:
    """
    Given the model used, display total tokens used and estimated expense
    """
    print(
        f"\nTotal tokens used: {context.prompt_tokens + context.completion_tokens}",
        file=sys.stderr,
    )

    if context.model in MODEL_INFO:
        total_expense = calculate_expense(
            context.prompt_tokens,
            context.completion_tokens,
            MODEL_INFO[context.model]["prompt"],
            MODEL_INFO[context.model]["completion"],
        )
        print(f"Estimated expense: ${total_expense}", file=sys.stderr)
    else:
        print(
            f"No expense estimate available for model {context.model}",
            file=sys.stderr,
        )


def run_prompt(context: ChatContext, message: str) -> None:
    """Ask the user for input, build the request and perform it."""
    context.messages.append({"role": "user", "content": message})

    stream = context.client.chat.completions.create(
        model=context.model,
        messages=context.messages,
        temperature=context.temperature,
        stream=True,
    )

    try:
        output = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                print(chunk.choices[0].delta.content, end="", flush=True)
                output += chunk.choices[0].delta.content
        context.messages.append({"role": "assistant", "content": output})
    except KeyboardInterrupt:
        pass
    finally:
        stream.response.close()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c", "--context", nargs="+", help="Additional context"
    )
    parser.add_argument(
        "-C",
        "--context-path",
        type=Path,
        nargs="+",
        help="Path to a context file",
    )
    parser.add_argument("-m", "--model", help="Set the model")
    parser.add_argument(
        "-t",
        "--temperature",
        type=float,
        help="Set the temperature (default: 1)",
        default=1.0,
    )
    parser.add_argument(
        "-ml",
        "--multiline",
        action="store_true",
        help="Use the multiline input mode",
    )
    return parser.parse_args()


import select
import sys


def stdin_has_data():
    return select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], [])


def run_interactive(context: ChatContext, args: argparse.Namespace) -> None:
    history = FileHistory(HISTORY_FILE)
    session = PromptSession(history=history, multiline=args.multiline)

    print("ChatGPT CLI", file=sys.stderr)
    print(f"Model in use: {context.model}", file=sys.stderr)

    atexit.register(display_expense, context=context)

    while True:
        try:
            print(file=sys.stderr)
            message = session.prompt(
                HTML(
                    f"<b>[{context.prompt_tokens + context.completion_tokens}] >>> </b>"
                )
            )

            if message.lower() == ".exit":
                raise EOFError
            if message.lower() == "":
                raise KeyboardInterrupt

            run_prompt(context, message)
            print(file=sys.stderr)
            print(file=sys.stderr)
        except KeyboardInterrupt:
            continue
        except EOFError:
            break
    save_history(context)


def run_non_interactive(
    context: ChatContext, args: argparse.Namespace
) -> None:
    if args.multiline:
        run_prompt(context, sys.stdin.read())
    else:
        for line in sys.stdin.readlines():
            run_prompt(context, line)

    save_history(context)


def main() -> None:
    args = parse_args()

    try:
        config = load_config(CONFIG_FILE)
    except FileNotFoundError:
        print("Configuration file not found", file=sys.stderr)
        sys.exit(1)

    if args.model:
        config["model"] = args.model
    if args.temperature is not None:
        config["temperature"] = args.temperature

    messages: list[Message] = []
    if args.context_path:
        for path in args.context_path:
            print(f"Context file: {path.name}", file=sys.stderr)
            messages.append(
                {"role": "system", "content": path.read_text().strip()}
            )
    elif args.context:
        for text in args.context:
            print(f"Context: {text}", file=sys.stderr)
            messages.append({"role": "system", "content": text.strip()})

    context = ChatContext(
        client=OpenAI(api_key=config["api-key"]),
        model=config["model"],
        temperature=config["temperature"],
        messages=messages,
    )

    if stdin_has_data():
        run_non_interactive(context, args)
    else:
        run_interactive(context, args)


if __name__ == "__main__":
    main()
