#!/usr/bin/python3
# Pulls tags to yume.pl from other boorus

import os
import sys
import re
import json
import argparse
import requests
from bs4 import BeautifulSoup
from dotfiles import iqdb
from dotfiles import config
from dotfiles import logging

logger = logging.getLogger(__name__)
config_key = config['yume-autotagger']

HISTORY_PATH = os.path.expanduser('~/.config/yume-autotagger.json')
YUME_URL = config_key.get('url', 'https://yume.pl')
API_URL = config_key.get('api-url', 'https://yume.pl/api')
YUME_USER_NAME = config_key['user']
YUME_PASSWORD = config_key['password']


def sanitize_tag(name):
    return re.sub(r'[<>/]', '_', name)


def capitalize(name):
    return re.sub(r'(^|[_()])([a-z])', lambda m: m.group(0).upper(), name)


class Tag():
    def __init__(self, name, category=None):
        self.name = name
        self.category = category

    def __repr__(self):
        return 'Tag(%r, %r)' % (self.name, self.category)


class TagList():
    def __init__(self, tags=None):
        self._tags = tags or []

    def add(self, tag):
        if not self.has(tag):
            self._tags.append(tag)

    def delete(self, tag):
        self._tags = [
            t for t in self._tags if t.name.lower() != tag.name.lower()]

    def has(self, tag):
        return tag.name.lower() in [t.name.lower() for t in self._tags]

    def __iter__(self):
        return iter(self._tags)

    def __len__(self):
        return len(self._tags)

    def __repr__(self):
        return 'TagList(%r)' % self._tags


def get_tags_from_gelbooru(url):
    if 'gelbooru' not in url:
        raise ValueError('Not a valid Gelbooru URL')
    session = requests.Session()
    response = session.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    list_items = soup.select('#tag-sidebar li')
    for list_item in list_items:
        try:
            category = list_item['class'][0].replace('tag-type-', '')
            name = list_item.select('a')[1].text.replace(' ', '_')
            yield Tag(name, category)
        except IndexError:
            pass


def get_tags_from_danbooru(url):
    match = re.search(r'danbooru.*?(\d+)', url)
    if not match:
        raise ValueError('Not a valid Danbooru URL')
    post_id = int(match.group(1))
    session = requests.Session()
    response = session.get('http://danbooru.donmai.us/posts/%s.json' % post_id)
    post = json.loads(response.text)
    for category in ['artist', 'character', 'copyright', 'general']:
        for name in post['tag_string_' + category].split(' '):
            if name:
                yield Tag(name, category)


def parse_args():
    parser = argparse.ArgumentParser(
        description='Pulls tags from Gelbooru to Szurubooru')
    parser.add_argument(
        metavar='POST_ID', dest='post_ids', nargs='*', type=int,
        help=(
            'ID of the post to edit the tags for. ' +
            'If not specified, use latest unprocessed post'))
    parser.add_argument(
        '--force', action='store_true',
        help='Pull tags for already processed posts (used with POST_ID)')
    parser.add_argument(
        '--untagged', action='store_true',
        help=(
            'Pull tags only for posts that were not yet ' +
            'tagged (used with POST_ID)'))
    return parser.parse_args()


def collect_external_tags(post_url):
    source_urls = [
        result.url
        for result in iqdb.search(post_url)
        if result.similarity >= 0.90]

    if not source_urls:
        raise RuntimeError('Post not found on IQDB')

    handlers = [
        get_tags_from_danbooru,
        get_tags_from_gelbooru,
    ]

    for handler in handlers:
        for source_url in source_urls:
            try:
                return list(handler(source_url))
            except (ValueError, NotImplementedError):
                pass

    raise RuntimeError('Post found on IQDB, but not on any known site')


def create_session():
    session = requests.Session()
    session.keep_alive = False
    session.auth = requests.auth.HTTPBasicAuth(YUME_USER_NAME, YUME_PASSWORD)
    session.headers['Content-Type'] = 'application/json'
    session.headers['Accept'] = 'application/json'
    response = session.get(
        API_URL + '/user/' + YUME_USER_NAME + '?bump-login')
    if response.status_code != 200:
        logger.error(json.loads(response.text)['description'])
        return None
    return session


def pull_tags(session, post_id):
    response = session.get(API_URL + '/post/' + str(post_id))
    if response.status_code != 200:
        logger.error(json.loads(response.text)['description'])
        return False
    post = json.loads(response.text)
    post_url = post['contentUrl']
    logger.info('%s/post/%d', YUME_URL, post['id'])
    logger.info(post_url)

    new_tags = TagList()
    target_tags = TagList()
    existing_tags = TagList()
    for item in post['tags']:
        tag = Tag(item)
        target_tags.add(tag)
        existing_tags.add(tag)

    external_tags = collect_external_tags(post_url)

    for external_tag in external_tags:
        banned = external_tag.name in config_key['banned']
        if not banned:
            for banned_regex in config_key['banned_regex']:
                if re.match(banned_regex, external_tag.name, re.I):
                    banned = True
                    break

        if banned:
            logger.warning('Ignored tag %r', external_tag.name)
            continue

        if external_tag.name in config_key['translations']:
            logger.info(
                'Translating tag %r -> %r',
                external_tag.name,
                config_key['translations'][external_tag.name])
            external_tag.name = config_key['translations'][external_tag.name]

        sanitized_name = sanitize_tag(external_tag.name)
        if sanitized_name != external_tag.name:
            logger.info(
                'Sanitized %r -> %r',
                external_tag.name,
                sanitized_name)
            external_tag.name = sanitized_name

        if target_tags.has(external_tag):
            continue

        target_tags.add(external_tag)

        response = session.get(API_URL + '/tag/' + external_tag.name)
        if response.status_code == 200:
            logger.info('Added existing tag %r', external_tag.name)
            upstream_tag = json.loads(response.text)
            for implication in upstream_tag['implications']:
                implication = Tag(implication)
                if not target_tags.has(implication):
                    target_tags.add(implication)
                    logger.info('Added implication %r', implication.name)
        elif response.status_code == 404 \
                and 'not found' in \
                    json.loads(response.text)['description'].lower():
            logger.warning('Added new tag %r', external_tag.name)
            new_tags.add(external_tag)
        else:
            logger.error(response.text, response.status_code)

    num_tags_added = len(target_tags) - len(existing_tags)
    if num_tags_added <= 0:
        logger.success('No new tags.')
        return True

    if target_tags.has(Tag('tagme')):
        logger.info('Deleting tag %r', 'tagme')
        target_tags.delete(Tag('tagme'))

    response = session.put(
        API_URL + '/post/' + str(post_id),
        data=json.dumps({
            'tags': [tag.name for tag in target_tags],
            'version': post['version'],
        }))
    if response.status_code != 200:
        logger.error(response.text, response.status_code)

    for new_tag in new_tags:
        response = session.get(API_URL + '/tag/' + new_tag.name)
        if response.status_code != 200:
            logger.error('Upstream tag ' + new_tag.name + ' was not created')
            continue
        upstream_tag = json.loads(response.text)
        if new_tag.category in ('artist', 'copyright', 'character'):
            new_tag.name = capitalize(new_tag.name)
        response = session.put(
            API_URL + '/tag/' + new_tag.name,
            data=json.dumps({
                'names': [new_tag.name],
                'category': config_key['categories'][new_tag.category],
                'version': upstream_tag['version'],
            }))
        if response.status_code != 200:
            logger.error(response.text, response.status_code)

    logger.success('Added %d new tags.', num_tags_added)
    return True


class TaggingHistory():
    def __init__(self):
        self._history = {'tagged': set(), 'untagged': set(), 'unknown': set()}
        self._load()

    def mark_as_tagged(self, post_id):
        self._history['tagged'].add(post_id)
        self._save()

    def mark_as_untagged(self, post_id):
        self._history['untagged'].add(post_id)
        self._save()

    def get_processed_post_ids(self):
        return self._history['tagged'] \
            | self._history['untagged'] \
            | self._history.get('unknown', set())

    def get_tagged_post_ids(self):
        return self._history['tagged']

    def get_untagged_post_ids(self):
        return self._history['untagged']

    def _load(self):
        if os.path.exists(HISTORY_PATH):
            with open(HISTORY_PATH, 'r') as handle:
                result = json.load(handle)
                self._history = {}
                for key in ['tagged', 'untagged', 'unknown']:
                    self._history[key] = set(result.get(key, []))

    def _save(self):
        result = {}
        for key in ['tagged', 'untagged', 'unknown']:
            result[key] = sorted(list(self._history[key]))
        with open(HISTORY_PATH, 'w') as handle:
            json.dump(result, handle)


def get_last_post_id(session, history):
    response = session.get(API_URL + '/posts?fields=id')
    decoded_response = json.loads(response.text)
    latest_id = decoded_response['results'][0]['id']
    processed_post_ids = history.get_processed_post_ids()
    for post_id in range(1, latest_id + 1):
        if post_id not in processed_post_ids:
            return post_id
    return None


def main():
    args = parse_args()
    session = create_session()
    if not session:
        logger.error('Failed to log in')
        return
    history = TaggingHistory()

    post_ids = []
    if args.post_ids:
        post_ids = args.post_ids
    else:
        last_post_id = get_last_post_id(session, history)
        if not last_post_id:
            logger.success('All posts up to date')
            return
        post_ids = [last_post_id]

    all_succeeded = True
    processed_post_ids = list(history.get_processed_post_ids())
    tagged_post_ids = list(history.get_tagged_post_ids())
    for post_id in post_ids:
        should_process = \
            post_id not in processed_post_ids \
            or args.force \
            or (post_id not in tagged_post_ids and args.untagged)

        if not should_process:
            logger.info('Skipping post %d - already processed', post_id)
            continue

        logger.info('Getting post %d...', post_id)
        try:
            local_succeded = pull_tags(session, post_id)
            history.mark_as_tagged(post_id)
        except ConnectionError as ex:
            logger.error(str(ex))
            local_succeded = False
        except Exception as ex:
            logger.error(str(ex))
            local_succeded = False
            history.mark_as_untagged(post_id)

        all_succeeded = all_succeeded and local_succeded

    sys.exit(0 if all_succeeded else 1)


if __name__ == '__main__':
    main()
