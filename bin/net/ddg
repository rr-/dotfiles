#!/usr/bin/env python3
import argparse
import typing as t
import requests
import urllib.parse
from bs4 import BeautifulSoup


def fish_link(query: str) -> str:
    return urllib.parse.parse_qs(query)['uddg'][0]


def search(query: str) -> t.Iterable[t.Tuple[str, str]]:
    response = requests.get(
        'https://duckduckgo.com/html/', params={'q': query})
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'lxml')
    visited: t.Set[str] = set()
    for link in soup.find_all('a'):
        try:
            url = fish_link(link['href'])
        except KeyError:
            continue
        text = link.text.strip()

        if url not in visited and text:
            yield (url, link.text.strip())
            visited.add(url)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument('query', nargs='+')
    parser.add_argument('-l', '--limit', type=int, default=5)
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    query = ' '.join(args.query or [])
    for i, (url, text) in enumerate(search(query)):
        print(url, text)
        if i >= args.limit:
            break


if __name__ == '__main__':
    main()
