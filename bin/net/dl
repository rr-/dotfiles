#!/usr/bin/python3
# Downloads things from the Internet with the power of regexes

from queue import Queue, LifoQueue
import argparse
import os
import pickle
import re
import sys
import threading
import time
import urllib.parse
import signal
import lockfile
import requests
from bs4 import BeautifulSoup


def run_workers(queue, count, worker_factory):
    ''' Executes all the given workers and blocks until the queue's empty. '''
    for _ in range(count):
        worker = worker_factory()
        thread = threading.Thread(target=worker.run)
        thread.daemon = True
        thread.start()
    while not Flow.terminated and queue.unfinished_tasks:
        time.sleep(1)


class Flow():
    ''' Used to control the lifetime of the script. '''
    terminated = False

    @staticmethod
    def terminate():
        Flow.terminated = True


class History(set):
    '''
    Keeps history of downloaded files so that the script doesn't download
    the same thing twice between runs.
    '''
    def __init__(self, path):
        super().__init__()
        self.path = path
        self.lock = None
        self.handle = None

    def __enter__(self):
        self.handle = None
        if self.path and os.path.exists(self.path):
            self.lock = lockfile.LockFile(self.path)
            self.lock.acquire(timeout=-1)
            self.handle = open(self.path, 'r+b')
            self.update(pickle.load(self.handle))
            print('Loaded %d URLs from history file' % len(self))
        return self

    def __exit__(self, *args):
        if not self.handle:
            return
        self.handle.truncate(0)
        self.handle.seek(0)
        copy = self.copy()
        pickle.dump(copy, self.handle)
        self.handle.close()
        self.lock.release()
        print('Saved %d URLs to history file' % len(copy))


class BaseWorker():
    '''
    A base for all the workers that contains basic execution flow, outlined to
    fulfill DRY.
    '''
    num = 0

    def __init__(self, queue, lock):
        self.queue = queue
        self.lock = lock
        self.num = BaseWorker.num
        BaseWorker.num += 1

    def run(self):
        '''
        Tries to take items from the queue and process them, handling
        gracefully possible errors.
        '''
        while not Flow.terminated:
            item = self.queue.get()
            try:
                status = self.process(item)
            except Exception as ex:
                status = self.on_error(item, ex)
            finally:
                self.queue.task_done()
            with self.lock:
                print('[T%d ~%5d queued] %s' % (
                    self.num,
                    self.queue.qsize(),
                    self.format_status(item, status)))

    def process(self, item):
        raise NotImplementedError()

    def on_error(self, item, ex):
        raise NotImplementedError()

    def format_status(self, item, status):
        raise NotImplementedError()


class Command():
    ''' The action to undertake, chosen via CLI '''

    @staticmethod
    def decorate_parser(parent_parser, fmt):
        '''
        Used to collect any extra CLI arguments specific to this command.
        '''
        raise NotImplementedError()

    def run(self, args):
        ''' Executed when the user chooses given command via CLI '''
        raise NotImplementedError()


class DownloadCommand(Command):
    '''
    This command contains implementation details on how to actually
    download things.
    '''

    @staticmethod
    def decorate_parser(parent_parser, fmt):
        parser = parent_parser.add_parser(
            'download', help='download files',
            formatter_class=fmt, aliases=['dl'])

        parser.add_argument(
            '-a', '--accept', metavar='REGEX', default='.*',
            help='set regex indicating which URLs to crawl')

        parser.add_argument(
            '-H', '--history', metavar='FILE',
            help='set path to the history file')

        parser.add_argument(
            '-t', '--target', metavar='DIR', default='.',
            help='set base target directory')

        parser.add_argument(
            '-r', '--retries', metavar='NUM', type=int, default=3,
            help='set retry count for failed downloads')

        parser.add_argument(
            '--retry_wait', metavar='NUM', type=int, default=3,
            help='set retry count for failed downloads')

        parser.add_argument(
            '-n', '--num', dest='workers', metavar='NUMBER', type=int,
            default=1, help='set worker count')

        parser.add_argument(
            'url', metavar='URL', nargs='+',
            help='initial URLs to retrieve')

        parser.set_defaults(command=DownloadCommand)

    def run(self, args):
        args.accept = re.compile(args.accept)
        visited = set()
        with History(args.history) as history:
            queue = LifoQueue()
            for url in args.url:
                visited.add(url)
                queue.put(DownloadCommand.Item(url))
            lock = threading.Lock()
            run_workers(
                queue,
                args.workers,
                lambda: DownloadCommand._Worker(
                    queue, lock, args, visited, history))

    class _Worker(BaseWorker):
        def __init__(self, queue, lock, args, visited, history):
            super().__init__(queue, lock)
            self.args = args
            self.visited = visited
            self.history = history

        def process(self, item):
            response = requests.get(item.url, timeout=(3, 3))
            if response.status_code != 200:
                raise RuntimeError('HTTP error %d' % response.status_code)
            mime = response.headers['content-type'].split(';')[0].lower()
            if mime == 'text/html':
                return self._process_html(response)
            else:
                return self._process_nonhtml(response)

        def _process_html(self, response):
            soup = BeautifulSoup(response.text, 'html.parser')
            child_urls = set()
            for link in soup.find_all('a', href=True):
                child_url = urllib.parse.urldefrag(
                    urllib.parse.urljoin(response.url, link['href'])).url
                with self.lock:
                    if self.args.accept.search(child_url) \
                            and child_url not in self.visited \
                            and child_url not in self.history:
                        child_urls.add(child_url)
                        self.visited.add(child_url)
            with self.lock:
                for child_url in sorted(child_urls):
                    self.queue.put(DownloadCommand.Item(child_url))
                    self.visited.add(child_url)
            return self.Result(urls_added=len(child_urls))

        def _process_nonhtml(self, response):
            parsed_url = urllib.parse.urlparse(response.url)
            target_path = os.path.join(
                self.args.target,
                parsed_url.netloc,
                re.sub(r'^[\/]*', '', parsed_url.path))
            target_dir = os.path.dirname(target_path)
            os.makedirs(target_dir, exist_ok=True)
            with open(target_path, 'wb') as handle:
                handle.write(response.content)
            with self.lock:
                self.history.add(response.url)
            return self.Result(download_path=target_path)

        def on_error(self, item, ex):
            time.sleep(self.args.retry_wait)
            item.retries += 1
            if item.retries < self.args.retries:
                self.queue.put(item)
            return self.Result(error=str(ex))

        def format_status(self, item, status):
            messages = []
            if status.error:
                messages.append('error: %s' % status.error)
            if status.download_path:
                messages.append('saved to %s' % status.download_path)
            if status.urls_added is not None:
                messages.append('added %d URLs' % status.urls_added)
            if item.retries > 1:
                messages.append('retry #%d' % item.retries)
            return '%s: %s' % (item.url, '; '.join(messages))

        class Result():
            def __init__(
                    self, download_path=None, urls_added=None, error=None):
                self.download_path = download_path
                self.urls_added = urls_added
                self.error = error

    class Item():
        def __init__(self, url):
            self.url = url
            self.retries = 0


class PruneCommand(Command):
    '''
    This command optimizes the history file, by checking for each URL if it
    returns 404. Such entries are removed from the history. All other entries
    are kept (even if they report errors such as 503).
    '''

    @staticmethod
    def decorate_parser(parent_parser, fmt):
        parser = parent_parser.add_parser(
            'prune', help='prune old entries in history file',
            formatter_class=fmt)

        parser.add_argument(
            '-H', '--history', metavar='FILE',
            help='set path to the history file')

        parser.add_argument(
            '-n', '--num', dest='workers', metavar='NUMBER', type=int,
            default=1, help='set worker count')

        parser.set_defaults(command=PruneCommand)

    def run(self, args):
        with History(args.history) as history:
            queue = Queue()
            for url in history:
                queue.put(url)
            lock = threading.Lock()
            run_workers(
                queue,
                args.workers,
                lambda: PruneCommand._Worker(queue, lock, history))

    class _Worker(BaseWorker):
        def __init__(self, queue, lock, history):
            super().__init__(queue, lock)
            self.history = history

        def process(self, item):
            response = requests.head(item.strip())
            if response.status_code == 404:
                with self.lock:
                    self.history.remove(item)
                return 'pruned (404)'
            return 'spared (%d)' % response.status_code

        def on_error(self, item, ex):
            return 'error: %s' % str(ex)

        def format_status(self, item, status):
            return '%s: %s' % (item.strip(), status)


def parse_args():
    ''' Collects all the option from the CLI arguments '''
    def fmt(prog):
        return argparse.HelpFormatter(prog, max_help_position=40, width=80)

    parser = argparse.ArgumentParser(
        description='Fetch things from the Internet', formatter_class=fmt)
    subparsers = parser.add_subparsers(
        help='choose the command', dest='command')
    DownloadCommand.decorate_parser(subparsers, fmt)
    PruneCommand.decorate_parser(subparsers, fmt)
    subparsers.required = True
    return parser.parse_args()


def main():
    args = parse_args()

    def signal_handler(_signal, _frame):
        if not Flow.terminated:
            sys.stdout.write('Exiting due to user abort\n')
            Flow.terminate()
    signal.signal(signal.SIGINT, signal_handler)

    command = args.command()
    command.run(args)


if __name__ == '__main__':
    main()
